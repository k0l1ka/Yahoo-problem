{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spag.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPeA6j7gDlEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import multiprocessing as mp\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "from tqdm import tqdm\n",
        "import decimal\n",
        "%matplotlib inline\n",
        "D = decimal.Decimal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYt-rPqMpS2A",
        "colab_type": "text"
      },
      "source": [
        "#These paramaters refer to RCV dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-VmdAM6pXCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number_of_features = 47236\n",
        "number_of_objects = 20242\n",
        "# number_of_objects = 5000\n",
        "server_data_file = 'server_data_sampled.txt'\n",
        "train_data_file = 'rcv1_train.txt'\n",
        "test_data_file = 'rcv1_test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGk_h0LOo_nM",
        "colab_type": "text"
      },
      "source": [
        "#Mathematical functions needed for SPAG algorithm, SPAG itself and uxiliary methods are implemented here.\n",
        "    1 - phi function may be altered to receive a list of arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zsusvv92cwan",
        "colab": {}
      },
      "source": [
        "\n",
        "def logit(z):\n",
        "    return 1 / (1 + np.exp( (-1) * z) )\n",
        "\n",
        "def quad_eq_roots(a,b,c):\n",
        "    d = (b**2) - (4*a*c)\n",
        "    sol1 = ((-1) * b - np.sqrt(d)) / (2 * a)\n",
        "    sol2 = ((-1) * b + np.sqrt(d)) / (2 * a)\n",
        "    if sol2 > sol1:\n",
        "        if sol2 <= 0:\n",
        "            print('oh J - 1') \n",
        "        return sol2\n",
        "    else:\n",
        "        if sol1 <= 0:\n",
        "            print('oh J - 2') \n",
        "        return sol1\n",
        "\n",
        "def l(x, y, a, Lambda):\n",
        "    return float( (1 + D( (-1) * y * (x @ a) ).exp() ).ln() ) + 0.5 * Lambda *  np.linalg.norm(x)**2\n",
        "    \n",
        "\n",
        "def grad_l(x, y, a, Lambda):\n",
        "    d = D( (-1) * y * (x @ a) ).exp() \n",
        "    return Lambda * x + float(d / (1 + d)) * (-1) * y * a\n",
        "\n",
        "\n",
        "def f_and_grad_f(x, data_file, Lambda):\n",
        "    func = 0\n",
        "    grad_f = np.zeros(number_of_features)\n",
        "    num_lines = 0\n",
        "    with open(data_file, 'r') as f:     \n",
        "        for line in f:\n",
        "            num_lines += 1\n",
        "            a = np.zeros(number_of_features)\n",
        "            for i, item in enumerate(line.split(' ')):\n",
        "                if i != 0:\n",
        "                    a[int(item.split(':')[0]) - 1] = float(item.split(':')[1])\n",
        "                else:\n",
        "                    y = int(item)\n",
        "            one_point_loss = l(x, y, a, Lambda)\n",
        "            func += one_point_loss\n",
        "            one_point_grad_loss = grad_l(x, y, a, Lambda)\n",
        "            grad_f += one_point_grad_loss\n",
        "    return func / num_lines , grad_f / num_lines\n",
        "\n",
        "\n",
        "def phi_and_grad_phi(x, Lambda):\n",
        "    f, grad_f = f_and_grad_f(x, server_data_file, Lambda)\n",
        "    phi = f + 0.5 * Mu * (np.linalg.norm(x))**2\n",
        "    grad_phi = grad_f + Mu * x\n",
        "    return phi, grad_phi\n",
        "\n",
        "\n",
        "def V_and_grad_V(x, grad_F, eta, beta, v, y, Lambda):\n",
        "    phi_x, grad_phi_x = phi_and_grad_phi(x, Lambda)\n",
        "    phi_y, grad_phi_y = phi_and_grad_phi(y, Lambda)\n",
        "    phi_v, grad_phi_v = phi_and_grad_phi(v, Lambda)\n",
        "\n",
        "    V = eta * grad_F @ x + (1 - beta) * (phi_x - phi_v - grad_phi_v @ (x-v) ) + beta * (phi_x - phi_y - grad_phi_y @ (x-y) )\n",
        "    grad_V = eta * grad_F + grad_phi_x - beta * grad_phi_y - (1 - beta) * grad_phi_v\n",
        "    return V, grad_V \n",
        "\n",
        "\n",
        "def SPAG(L, sigma, num_iter, x0):\n",
        "    x = np.empty([num_iter + 1, number_of_features])\n",
        "\n",
        "    x[0] = x0\n",
        "    v = x0\n",
        "    A = 0\n",
        "    B = 1\n",
        "    G = 1\n",
        "\n",
        "    with mp.Pool(processes = num_machines) as pool: \n",
        "        for t in range(num_iter):           \n",
        "            print('iteration ', t)   \n",
        "            G = 0.5 * max(1, 0.5 * G)\n",
        "            \n",
        "            print('round 0')\n",
        "            \n",
        "            G = 2 * G\n",
        "            a = quad_eq_roots(sigma - L * G, sigma * A + B, A * B)\n",
        "            A = A + a\n",
        "            B = B + a * sigma\n",
        "            alpha = a / A \n",
        "            beta = a / B * sigma\n",
        "            eta = a / B\n",
        "\n",
        "            y = ( (1 - alpha) * x[t] + alpha * (1 - beta) * v ) / (1 - alpha * beta)\n",
        "            \n",
        "            print('start gradient in parallel')\n",
        "            results = pool.starmap_async(f_and_grad_f, [(y, 'data_' + str(n+1) + '.txt', Lambda) for n in range(num_machines)])\n",
        "            funcs_and_grads = results.get()\n",
        "\n",
        "            grad_F_at_y = 0\n",
        "            for item in funcs_and_grads:\n",
        "                grad_F_at_y += item[1]\n",
        "            grad_F_at_y = grad_F_at_y / num_machines\n",
        "            print('finished gradient in parallel')\n",
        "    \n",
        "            print('started min V')\n",
        "            v_next, min_V_t, info_V = fmin_l_bfgs_b(func= V_and_grad_V, x0= v, fprime= None, args= [grad_F_at_y, eta, beta, v, y, Lambda], \n",
        "                                                approx_grad= False, bounds= [(None, None)] * number_of_features, maxiter= 10)\n",
        "            x[t+1] = (1 - alpha) * x[t] + alpha * v_next\n",
        "            print('finished min V ')\n",
        "\n",
        "            phi_x, grad_phi_x = phi_and_grad_phi(x[t+1], Lambda)\n",
        "            phi_y, grad_phi_y = phi_and_grad_phi(y, Lambda)\n",
        "            phi_v, grad_phi_v = phi_and_grad_phi(v, Lambda)\n",
        "            phi_v_next, grad_phi_v_next = phi_and_grad_phi(v_next, Lambda)\n",
        "            \n",
        "            D_phi_at_x_y = phi_x - phi_y - grad_phi_y @ (x[t+1] - y)\n",
        "            D_phi_at_v_next_v = phi_v_next - phi_v - grad_phi_v @ (v_next - v)\n",
        "            D_phi_at_v_next_y = phi_v_next - phi_y - grad_phi_y @ (v_next - y)\n",
        "\n",
        "            condition = D_phi_at_x_y <= alpha**2 * G * ( (1 - beta) * D_phi_at_v_next_v + beta * D_phi_at_v_next_y ) \n",
        "            \n",
        "            print('end round 0')\n",
        "            rount = 0\n",
        "            \n",
        "            while not condition:\n",
        "                rount += 1\n",
        "                print('round ', rount)\n",
        "                \n",
        "                G = 2 * G\n",
        "                a = quad_eq_roots(sigma - L * G, sigma * A + B, A * B)\n",
        "                A = A + a\n",
        "                B = B + a * sigma\n",
        "                alpha = a / A \n",
        "                beta = a / B * sigma\n",
        "                eta = a / B\n",
        "\n",
        "                y = ( (1 - alpha) * x[t] + alpha * (1 - beta) * v ) / (1 - alpha * beta)\n",
        "\n",
        "                print('start gradient in parallel')\n",
        "                results = pool.starmap_async(f_and_grad_f, [(y, 'data_' + str(n+1) + '.txt', Lambda) for n in range(num_machines)])\n",
        "                funcs_and_grads = results.get()\n",
        "\n",
        "                grad_F_at_y = 0\n",
        "                for item in funcs_and_grads:\n",
        "                    grad_F_at_y += item[1]\n",
        "                grad_F_at_y = grad_F_at_y / num_machines\n",
        "                print('finished gradient in parallel')\n",
        "\n",
        "        \n",
        "                print('started min V')\n",
        "                v_next, min_V_t, info_V = fmin_l_bfgs_b(func= V_and_grad_V, x0= v, fprime= None, args= [grad_F_at_y, eta, beta, v, y, Lambda], \n",
        "                                                    approx_grad= False, bounds= [(None, None)] * number_of_features, maxiter= 10)\n",
        "                x[t+1] = (1 - alpha) * x[t] + alpha * v_next\n",
        "                print('finished min V ')\n",
        "\n",
        "                phi_x, grad_phi_x = phi_and_grad_phi(x[t+1], Lambda)\n",
        "                phi_y, grad_phi_y = phi_and_grad_phi(y, Lambda)\n",
        "                phi_v, grad_phi_v = phi_and_grad_phi(v, Lambda)\n",
        "                phi_v_next, grad_phi_v_next = phi_and_grad_phi(v_next, Lambda)\n",
        "                \n",
        "                D_phi_at_x_y = phi_x - phi_y - grad_phi_y @ (x[t+1] - y)\n",
        "                D_phi_at_v_next_v = phi_v_next - phi_v - grad_phi_v @ (v_next - v)\n",
        "                D_phi_at_v_next_y = phi_v_next - phi_y - grad_phi_y @ (v_next - y)\n",
        "\n",
        "                condition = D_phi_at_x_y <= alpha**2 * G * ( (1 - beta) * D_phi_at_v_next_v + beta * D_phi_at_v_next_y ) \n",
        "                            \n",
        "                print('end round ', rount)\n",
        "\n",
        "            # print('x = {} , y = {} , v = {} , v_next = {}'.format(x[t+1], y, v, v_next))\n",
        "\n",
        "            v = v_next\n",
        "    return x\n",
        "\n",
        "\n",
        "    \n",
        "def indices_for_server(number_to_sample):\n",
        "    labels = []\n",
        "    with open(train_data_file, 'r') as f:     \n",
        "        for i, line in enumerate(f):\n",
        "            if i < number_of_objects:\n",
        "                labels.append( int(line.split(' ')[0]) )\n",
        "    labels = np.array(labels)\n",
        "    indices_of_ones = np.where(labels == 1)[0]\n",
        "    num_ones_total = len(indices_of_ones)\n",
        "    num_ones_server = round(num_ones_total / number_of_objects * number_to_sample)\n",
        "    np.random.shuffle(indices_of_ones)\n",
        "    ones_for_server = indices_of_ones[0:int(num_ones_server - 1)]\n",
        "\n",
        "    indices_of_minus_ones = np.where(labels == -1)[0]\n",
        "    num_minus_ones_total = len(indices_of_minus_ones)\n",
        "    num_minus_ones_server = number_to_sample - num_ones_server\n",
        "    np.random.shuffle(indices_of_minus_ones)\n",
        "    minus_ones_for_server = indices_of_minus_ones[0:int(num_minus_ones_server - 1)]\n",
        "\n",
        "    server_indices = np.concatenate([ones_for_server, minus_ones_for_server])\n",
        "    return server_indices\n",
        "\n",
        "\n",
        "def F(x, data_file, Lambda):\n",
        "    func = 0\n",
        "    num_lines = 0\n",
        "    with open(data_file, 'r') as f:     \n",
        "        for line in f:\n",
        "            num_lines += 1\n",
        "            a = np.zeros(number_of_features)\n",
        "            for i, item in enumerate(line.split(' ')):\n",
        "                if i != 0:\n",
        "                    a[int(item.split(':')[0]) - 1] = float(item.split(':')[1])\n",
        "                else:\n",
        "                    y = int(item)\n",
        "            one_point_loss = l(x, y, a, Lambda)\n",
        "            func += one_point_loss\n",
        "    return func / num_lines\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv6it36hSF7d",
        "colab_type": "text"
      },
      "source": [
        "#Methods for spliting data among workers \n",
        "\n",
        "(cluster with distributed memory is used)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkwiIP5XDx0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def work_for_slaves():\n",
        "    total_num_lines = number_of_objects\n",
        "    parts_length = []\n",
        "    batch = total_num_lines // num_machines\n",
        "    rest = total_num_lines % num_machines\n",
        "\n",
        "    for _ in range(num_machines): \n",
        "        parts_length.append(batch)\n",
        "\n",
        "    for i in range(rest):\n",
        "        parts_length[i] += 1\n",
        "\n",
        "    with open(train_data_file, 'r') as f:    \n",
        "        for i in range(num_machines):\n",
        "            with open('data_' + str(i+1) + '.txt', 'w') as file_local:    \n",
        "                for _ in range(parts_length[i]):\n",
        "                    file_local.write(f.readline())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOyLkI68pFw-",
        "colab_type": "text"
      },
      "source": [
        "Server will hold uniformly sampled data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXCDoOyepFQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def work_for_master(number_to_sample):\n",
        "    indices_to_select_for_server = indices_for_server(number_to_sample) \n",
        "\n",
        "    with open(train_data_file, 'r') as f:    \n",
        "        with open(server_data_file, 'w') as file_local:    \n",
        "            for i, line in enumerate(f):\n",
        "                if i in indices_to_select_for_server:\n",
        "                    file_local.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3BAhEdAfsOb",
        "colab_type": "text"
      },
      "source": [
        "#Methods to plot loss on iterations of optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ63krKGSbmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_F_at_iters(x_at_iterations, Lambda):\n",
        "    y = []\n",
        "    count = 0\n",
        "    with mp.Pool(processes = num_machines) as pool:\n",
        "        for item in x_at_iterations:\n",
        "            count += 1\n",
        "            print('Point ', count)\n",
        "            result = pool.starmap_async(F, [(item, 'data_' + str(n+1) + '.txt', Lambda) for n in range(num_machines)])\n",
        "            loss_values = result.get()\n",
        "            y.append(sum(loss_values) / num_machines)\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMVaG_srDz5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_F_at_iters_and_log(lambd, mu, sampled_data, y):\n",
        "    x = range(iterations_number + 1)\n",
        "    plt.style.use('seaborn-whitegrid')\n",
        "    plt.plot(x, y, 'o', color='blue');\n",
        "    plt.title('Loss over iters')\n",
        "    plt.xlabel(f'$iter$')\n",
        "    plt.ylabel(f'$Loss$')\n",
        "    # plt.legend(['1e9 points'], frameon = True)\n",
        "    plt.savefig('sampled_' + str(sampled_data) + '_lambda_' + str(Lambda) + '_mu_' + str(Mu) +'.png')\n",
        "    print('sampled_' + str(sampled_data) + '_lambda_' + str(Lambda) + '_mu_' + str(Mu) +'.png')\n",
        "    plt.show()\n",
        "    plt.clf()\n",
        "    with open('plot_data_logs-1.txt', 'a') as f:      \n",
        "        f.write('sampled ' + str(sampled_data) + ' lambda ' + str(lambd) + ' mu ' + str(mu) + ' ' + \n",
        "                ' '.join(map(str, y)) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0r7AURIf8VR",
        "colab_type": "text"
      },
      "source": [
        "#Hyper-parameters for RCV dataset  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy7NtMETf4D7",
        "colab_type": "code",
        "outputId": "ed33a89f-0e53-4807-a100-2e5db8adbdb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "num_machines = 20\n",
        "sampled_vectors_at_server = number_of_objects // 10\n",
        "\n",
        "L_F_phi = 1\n",
        "Mu = 1e-5\n",
        "Lambda = 1e-7\n",
        "sigma_F_phi = Lambda / (Lambda + 2 * Mu)\n",
        "\n",
        "iterations_number = 19\n",
        "\n",
        "\n",
        "'''\n",
        "Lambda_s = [1e-4, 1e-5, 1e-6, 1e-7, 1e-8]\n",
        "Mu_s = [1e-4, 1e-5, 1e-6]\n",
        "grades = [35, 30, 25, 20, 15, 10]\n",
        "'''\n",
        "# Lambda_s = [1e-8]\n",
        "# Mu_s = [1e-6]\n",
        "# grades = [10]\n",
        "\n",
        "# sample_grade = 10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nLambda_s = [1e-4, 1e-5, 1e-6, 1e-7, 1e-8]\\nMu_s = [1e-4, 1e-5, 1e-6]\\ngrades = [35, 30, 25, 20, 15, 10]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYxd2adBoMGs",
        "colab_type": "text"
      },
      "source": [
        "#Data ditribution \n",
        "\n",
        "(a cluster is modelled by several CPU cores each of that has access only to one datafile) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXf-d0M138-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "work_for_slaves()\n",
        "work_for_master(sampled_vectors_at_server)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJOGzDtT15bR",
        "colab_type": "text"
      },
      "source": [
        "#Grid search for hyper-parameters: mu, lambda and number of samples at the server\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N10-fTyWWQTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for Mu in Mu_s:\n",
        "    for Lambda in Lambda_s:\n",
        "        for sample_grade in grades:\n",
        "            sampled_vectors_at_server = number_of_objects // sample_grade\n",
        "            sigma_F_phi = Lambda / (Lambda + 2 * Mu)\n",
        "            work_for_master(sampled_vectors_at_server)\n",
        "            x_s = SPAG(L_F_phi, sigma_F_phi, iterations_number) \n",
        "            y_s = find_F_at_iters(x_s, Lambda)\n",
        "            plot_F_at_iters_and_log(Lambda, Mu, sampled_vectors_at_server, y_s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YiZ_rViCDz0",
        "colab_type": "text"
      },
      "source": [
        "##Start point x_start is the same for all next algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81ETOoP8CLAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_start, f_at_x_start, info = fmin_l_bfgs_b(func= f_and_grad_f, x0= np.zeros(number_of_features), fprime= None, \n",
        "                                        args= [server_data_file, Lambda], approx_grad= False, \n",
        "                                        bounds= [(None, None)] * number_of_features, maxiter= 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZQd5mbUSVGO",
        "colab_type": "text"
      },
      "source": [
        "#Applying Spag "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NIVxYArSRwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xxx_s = SPAG(L_F_phi, sigma_F_phi, iterations_number, x_start) \n",
        "ys_spag = find_F_at_iters(xxx_s, Lambda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x1diSfLUFkrH"
      },
      "source": [
        "##Applying Nesterov accelerated method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4tez1BraQJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install git+https://github.com/amkatrutsa/liboptpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41KwUgEQadHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import liboptpy.unconstr_solvers as solvers\n",
        "import liboptpy.step_size as ss\n",
        "import liboptpy.restarts as restarts\n",
        "import scipy.optimize as scopt\n",
        "import cvxpy\n",
        "\n",
        "def f(x):\n",
        "    return F(x, train_data_file, Lambda)\n",
        "\n",
        "def grad_f(x):\n",
        "    return f_and_grad_f(x, train_data_file, Lambda)[1]\n",
        "\n",
        "m_dict = { \"Nesterov\": solvers.fo.AcceleratedGD(f, grad_f, ss.ConstantStepSize(1e-3)) }\n",
        "\n",
        "tol = 1e-6\n",
        "key = \"Nesterov\"\n",
        "\n",
        "\n",
        "print(\"\\t {}\".format(key))\n",
        "q = m_dict[key].solve(x_start, iterations_number, tol, disp=True)\n",
        "\n",
        "# ys = [f(x) for x in m_dict[key].get_convergence()]\n",
        "\n",
        "# for key in m_dict:\n",
        "#     print(\"\\t {}\".format(key))\n",
        "#     %timeit m_dict[key].solve(x0, max_iter, tol)\n",
        "\n",
        "\n",
        "ys_agd = []\n",
        "count = 0\n",
        "with mp.Pool(processes = num_machines) as pool:\n",
        "    for item in m_dict[key].get_convergence():\n",
        "        count += 1\n",
        "        print('Point ', count)\n",
        "        result = pool.starmap_async(F, [(item, 'data_' + str(n+1) + '.txt', Lambda) for n in range(num_machines)])\n",
        "        loss_values = result.get()\n",
        "        ys_agd.append(sum(loss_values) / num_machines)\n",
        "\n",
        "\n",
        "# xs = range(1, iterations_number+2)\n",
        "# plt.style.use('seaborn-whitegrid')\n",
        "# plt.plot(xs, ys_agd, 'o', color='green')\n",
        "# plt.xlabel(\"Number of iteration\")\n",
        "# plt.ylabel(\"f(x_k)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-dN4hphMFysl"
      },
      "source": [
        "##Applying L-BFGS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsNZ9GmB659n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ys_lbfgs = []\n",
        "\n",
        "def callback_F(x):\n",
        "    with mp.Pool(processes = num_machines) as pool:\n",
        "        result = pool.starmap_async(F, [(x, 'data_' + str(n+1) + '.txt', Lambda) for n in range(num_machines)])\n",
        "        loss_values = result.get()\n",
        "        y = sum(loss_values) / num_machines\n",
        "    ys_lbfgs.append(y)\n",
        "\n",
        "\n",
        "ys_lbfgs.append( F(x_start, train_data_file, Lambda) )\n",
        "\n",
        "print('step 2')\n",
        "\n",
        "x_optimal, F_at_x_optimal, info2 = fmin_l_bfgs_b(func= f_and_grad_f, x0= x_start, fprime= None, args= [train_data_file, Lambda],\n",
        "                                                 approx_grad= False, bounds= [(None, None)] * number_of_features,\n",
        "                                                 maxiter= iterations_number, callback= callback_F)\n",
        "\n",
        "\n",
        "# xs = range(1, iterations_number + 2)\n",
        "# plt.style.use('seaborn-whitegrid')\n",
        "# plt.plot(xs, ys_lbfgs, 'o', color='red');    \n",
        "# plt.xlabel(\"Number of iteration\")\n",
        "# plt.ylabel(\"f(x_k)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLQtvm-_OYPo",
        "colab_type": "text"
      },
      "source": [
        "#Common graph for all algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LSh8H5nOX2g",
        "colab_type": "code",
        "outputId": "ca24af01-8cd0-4f09-c784-b56e01c6757c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "x = range(iterations_number + 1)\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "plt.title('Loss on iters')\n",
        "plt.xlabel(f'$iter$')\n",
        "plt.ylabel(f'$Loss$')\n",
        "plt.plot(x, ys_spag, 'o', color='red', label='SPAG')\n",
        "plt.plot(x, ys_agd, 'o', color='blue', label='AGD')\n",
        "plt.plot(x[:-2], ys_lbfgs, 'o', color='green', label='L-BFGS')\n",
        "plt.legend()\n",
        "plt.savefig('3|->sampled_' + str(sampled_vectors_at_server) + '_lambda_' + str(Lambda) + '_mu_' + str(Mu) +'.png')\n",
        "print('3|->sampled_' + str(sampled_vectors_at_server) + '_lambda_' + str(Lambda) + '_mu_' + str(Mu) +'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3|->sampled_2024_lambda_1e-07_mu_1e-05.png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhVdeMxW9zG1",
        "colab_type": "text"
      },
      "source": [
        "#Now let's compare the time of consequtive work of SPAG and L-BFGS with stop criterion -> pgtol = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xhkgf2F3Tuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def SPAG_with_timing_and_pgtol(L, sigma, x0, pgtol= 1e-5):\n",
        "    start = time.time()\n",
        "    \n",
        "    x = [x0]\n",
        "    x_prev = x0\n",
        "    x = np.array(x)\n",
        "    v = x0\n",
        "    A = 0\n",
        "    B = 1\n",
        "    G = 1\n",
        "\n",
        "    max_proj_grad_comp = 100\n",
        "    iteration = -1\n",
        "\n",
        "    with mp.Pool(processes = num_machines) as pool: \n",
        "        while max_proj_grad_comp > pgtol:      \n",
        "            iteration += 1\n",
        "            print('iteration ', iteration)   \n",
        "            G = 0.5 * max(1, 0.5 * G)\n",
        "            \n",
        "            print('start round 0')\n",
        "            \n",
        "            G = 2 * G\n",
        "            a = quad_eq_roots(sigma - L * G, sigma * A + B, A * B)\n",
        "            A = A + a\n",
        "            B = B + a * sigma\n",
        "            alpha = a / A \n",
        "            beta = a / B * sigma\n",
        "            eta = a / B\n",
        "\n",
        "            y = ( (1 - alpha) * x_prev + alpha * (1 - beta) * v ) / (1 - alpha * beta)\n",
        "            \n",
        "            results = pool.starmap_async(f_and_grad_f, [(y, 'data_' + str(n+1) + '.txt', Lambda) for n in range(num_machines)])\n",
        "            funcs_and_grads = results.get()\n",
        "\n",
        "            grad_F_at_y = 0\n",
        "            for item in funcs_and_grads:\n",
        "                grad_F_at_y += item[1]\n",
        "            grad_F_at_y = grad_F_at_y / num_machines\n",
        "    \n",
        "            v_next, min_V_t, info_V = fmin_l_bfgs_b(func= V_and_grad_V, x0= v, fprime= None, args= [grad_F_at_y, eta, beta, v, y, Lambda], \n",
        "                                                approx_grad= False, bounds= [(None, None)] * number_of_features)\n",
        "\n",
        "            x_next = (1 - alpha) * x_prev + alpha * v_next\n",
        "            \n",
        "\n",
        "            phi_x, grad_phi_x = phi_and_grad_phi(x_next, Lambda)\n",
        "            phi_y, grad_phi_y = phi_and_grad_phi(y, Lambda)\n",
        "            phi_v, grad_phi_v = phi_and_grad_phi(v, Lambda)\n",
        "            phi_v_next, grad_phi_v_next = phi_and_grad_phi(v_next, Lambda)\n",
        "            \n",
        "            D_phi_at_x_y = phi_x - phi_y - grad_phi_y @ (x_next - y)\n",
        "            D_phi_at_v_next_v = phi_v_next - phi_v - grad_phi_v @ (v_next - v)\n",
        "            D_phi_at_v_next_y = phi_v_next - phi_y - grad_phi_y @ (v_next - y)\n",
        "\n",
        "            condition = D_phi_at_x_y <= alpha**2 * G * ( (1 - beta) * D_phi_at_v_next_v + beta * D_phi_at_v_next_y ) \n",
        "            \n",
        "            print('end round 0')\n",
        "            rount = 0\n",
        "            \n",
        "            while not condition:\n",
        "                rount += 1\n",
        "                print('start round ', rount)\n",
        "                \n",
        "                G = 2 * G\n",
        "                a = quad_eq_roots(sigma - L * G, sigma * A + B, A * B)\n",
        "                A = A + a\n",
        "                B = B + a * sigma\n",
        "                alpha = a / A \n",
        "                beta = a / B * sigma\n",
        "                eta = a / B\n",
        "\n",
        "                y = ( (1 - alpha) * x_prev + alpha * (1 - beta) * v ) / (1 - alpha * beta)\n",
        "\n",
        "                results = pool.starmap_async(f_and_grad_f, [(y, 'data_' + str(n+1) + '.txt', Lambda) for n in range(num_machines)])\n",
        "                funcs_and_grads = results.get()\n",
        "\n",
        "                grad_F_at_y = 0\n",
        "                for item in funcs_and_grads:\n",
        "                    grad_F_at_y += item[1]\n",
        "                grad_F_at_y = grad_F_at_y / num_machines\n",
        "        \n",
        "                v_next, min_V_t, info_V = fmin_l_bfgs_b(func= V_and_grad_V, x0= v, fprime= None, args= [grad_F_at_y, eta, beta, v, y, Lambda], \n",
        "                                                    approx_grad= False, bounds= [(None, None)] * number_of_features)\n",
        "\n",
        "                # x = np.append(x, [(1 - alpha) * x[iteration] + alpha * v_next], axis= 0) \n",
        "                x_next = (1 - alpha) * x_prev + alpha * v_next\n",
        "\n",
        "\n",
        "                phi_x, grad_phi_x = phi_and_grad_phi(x_next, Lambda)\n",
        "                phi_y, grad_phi_y = phi_and_grad_phi(y, Lambda)\n",
        "                phi_v, grad_phi_v = phi_and_grad_phi(v, Lambda)\n",
        "                phi_v_next, grad_phi_v_next = phi_and_grad_phi(v_next, Lambda)\n",
        "                \n",
        "                D_phi_at_x_y = phi_x - phi_y - grad_phi_y @ (x_next - y)\n",
        "                D_phi_at_v_next_v = phi_v_next - phi_v - grad_phi_v @ (v_next - v)\n",
        "                D_phi_at_v_next_y = phi_v_next - phi_y - grad_phi_y @ (v_next - y)\n",
        "\n",
        "                condition = D_phi_at_x_y <= alpha**2 * G * ( (1 - beta) * D_phi_at_v_next_v + beta * D_phi_at_v_next_y ) \n",
        "                            \n",
        "                print('end round ', rount)\n",
        "\n",
        "            # print('x = {} , y = {} , v = {} , v_next = {}'.format(x[t+1], y, v, v_next))\n",
        "\n",
        "            v = v_next\n",
        "            x = np.append(x, [x_next], axis= 0)\n",
        "            x_prev = x_next\n",
        "            max_proj_grad_comp = max(abs( np.array(grad_F_at_y) ) )\n",
        "\n",
        "    end = time.time()\n",
        "    return x, end - start\n",
        "\n",
        "\n",
        "def dF(x1, x2, data_file, Lambda):\n",
        "    grad_f1 = np.zeros(number_of_features)\n",
        "    grad_f2 = np.zeros(number_of_features)\n",
        "    num_lines = 0\n",
        "    with open(data_file, 'r') as f:     \n",
        "        for line in f:\n",
        "            num_lines += 1\n",
        "            a = np.zeros(number_of_features)\n",
        "            for i, item in enumerate(line.split(' ')):\n",
        "                if i != 0:\n",
        "                    a[int(item.split(':')[0]) - 1] = float(item.split(':')[1])\n",
        "                else:\n",
        "                    y = int(item)\n",
        "            grad_f1 += grad_l(x1, y, a, Lambda)\n",
        "            grad_f2 += grad_l(x2, y, a, Lambda)\n",
        "    return grad_f1 / num_lines, grad_f2 / num_lines\n",
        "\n",
        "\n",
        "# Consecutive algorithm with stop criterion = pgtol and time measurement\n",
        "# New importnt feature in this part - minimization of V now has its pgtol stop criterion\n",
        "\n",
        "def SPAG_con(L, sigma, x0, V_norm_tol= 1e-2, pgtol= 1e-5):\n",
        "    start = time.time()\n",
        "    \n",
        "    x = [x0]\n",
        "    x_prev = x0\n",
        "    x = np.array(x)\n",
        "    v = x0\n",
        "    A = 0\n",
        "    B = 1\n",
        "    G = 1\n",
        "\n",
        "    max_proj_grad_comp = 100\n",
        "    iteration = -1\n",
        "\n",
        "    while max_proj_grad_comp > pgtol:      \n",
        "        iteration += 1\n",
        "        print('iteration ', iteration)   \n",
        "        G = 0.5 * max(1, 0.5 * G)\n",
        "        \n",
        "        print('start round 0')\n",
        "        \n",
        "        G = 2 * G\n",
        "        a = quad_eq_roots(sigma - L * G, sigma * A + B, A * B)\n",
        "        A = A + a\n",
        "        B = B + a * sigma\n",
        "        alpha = a / A \n",
        "        beta = a / B * sigma\n",
        "        eta = a / B\n",
        "\n",
        "        y = ( (1 - alpha) * x_prev + alpha * (1 - beta) * v ) / (1 - alpha * beta)\n",
        "        \n",
        "        print('начал градиент')\n",
        "        grad_F_at_x_prev, grad_F_at_y = dF(x_prev, y, train_data_file, Lambda)\n",
        "        print('закончил градиент')\n",
        "\n",
        "        print('начал min V')\n",
        "        v_next, min_V_t, info_V = fmin_l_bfgs_b(func= V_and_grad_V, x0= v, fprime= None, \n",
        "                                                args= [grad_F_at_y, eta, beta, v, y, Lambda], approx_grad= False,\n",
        "                                                bounds= [(None, None)] * number_of_features,\n",
        "                                                pgtol= V_norm_tol / np.sqrt(number_of_features), maxiter= 5)\n",
        "        print(info_V['nit'])\n",
        "        print('кончил min V ')\n",
        "        \n",
        "        x_next = (1 - alpha) * x_prev + alpha * v_next\n",
        "        \n",
        "\n",
        "        phi_x, grad_phi_x = phi_and_grad_phi(x_next, Lambda)\n",
        "        phi_y, grad_phi_y = phi_and_grad_phi(y, Lambda)\n",
        "        phi_v, grad_phi_v = phi_and_grad_phi(v, Lambda)\n",
        "        phi_v_next, grad_phi_v_next = phi_and_grad_phi(v_next, Lambda)\n",
        "        \n",
        "        D_phi_at_x_y = phi_x - phi_y - grad_phi_y @ (x_next - y)\n",
        "        D_phi_at_v_next_v = phi_v_next - phi_v - grad_phi_v @ (v_next - v)\n",
        "        D_phi_at_v_next_y = phi_v_next - phi_y - grad_phi_y @ (v_next - y)\n",
        "\n",
        "        condition = D_phi_at_x_y <= alpha**2 * G * ( (1 - beta) * D_phi_at_v_next_v + beta * D_phi_at_v_next_y ) \n",
        "        \n",
        "        print('end round 0')\n",
        "        rount = 0\n",
        "        \n",
        "        while not condition:\n",
        "            rount += 1\n",
        "            print('start round ', rount)\n",
        "            \n",
        "            G = 2 * G\n",
        "            a = quad_eq_roots(sigma - L * G, sigma * A + B, A * B)\n",
        "            A = A + a\n",
        "            B = B + a * sigma\n",
        "            alpha = a / A \n",
        "            beta = a / B * sigma\n",
        "            eta = a / B\n",
        "\n",
        "            y = ( (1 - alpha) * x_prev + alpha * (1 - beta) * v ) / (1 - alpha * beta)\n",
        "             \n",
        "            grad_F_at_x_prev, grad_F_at_y = dF(x_prev, y, train_data_file, Lambda)\n",
        "    \n",
        "            v_next, min_V_t, info_V = fmin_l_bfgs_b(func= V_and_grad_V, x0= v, fprime= None, \n",
        "                                                args= [grad_F_at_y, eta, beta, v, y, Lambda], approx_grad= False,\n",
        "                                                bounds= [(None, None)] * number_of_features,\n",
        "                                                pgtol= V_norm_tol / np.sqrt(number_of_features), maxiter= 5)\n",
        "            print(info_V['nit'])\n",
        "            # x = np.append(x, [(1 - alpha) * x[iteration] + alpha * v_next], axis= 0) \n",
        "            x_next = (1 - alpha) * x_prev + alpha * v_next\n",
        "\n",
        "\n",
        "            phi_x, grad_phi_x = phi_and_grad_phi(x_next, Lambda)\n",
        "            phi_y, grad_phi_y = phi_and_grad_phi(y, Lambda)\n",
        "            phi_v, grad_phi_v = phi_and_grad_phi(v, Lambda)\n",
        "            phi_v_next, grad_phi_v_next = phi_and_grad_phi(v_next, Lambda)\n",
        "            \n",
        "            D_phi_at_x_y = phi_x - phi_y - grad_phi_y @ (x_next - y)\n",
        "            D_phi_at_v_next_v = phi_v_next - phi_v - grad_phi_v @ (v_next - v)\n",
        "            D_phi_at_v_next_y = phi_v_next - phi_y - grad_phi_y @ (v_next - y)\n",
        "\n",
        "            condition = D_phi_at_x_y <= alpha**2 * G * ( (1 - beta) * D_phi_at_v_next_v + beta * D_phi_at_v_next_y ) \n",
        "                        \n",
        "            print('end round ', rount)\n",
        "\n",
        "        # print('x = {} , y = {} , v = {} , v_next = {}'.format(x[t+1], y, v, v_next))\n",
        "\n",
        "        v = v_next\n",
        "        x = np.append(x, [x_next], axis= 0)\n",
        "        x_prev = x_next\n",
        "        max_proj_grad_comp = max(abs( np.array(grad_F_at_x_prev) ) )\n",
        "\n",
        "    end = time.time()\n",
        "    return x, end - start\n",
        "\n",
        "\n",
        "def stack_steps(x):\n",
        "    global xs_lbfgs\n",
        "    xs_lbfgs.append(x)\n",
        "    print('iteration ', len(xs_lbfgs) - 1)\n",
        "    # with mp.Pool(processes = num_machines) as pool:\n",
        "    #     result = pool.starmap_async(F, [(x, 'data_' + str(n+1) + '.txt', Lambda) for n in range(num_machines)])\n",
        "    #     loss_values = result.get()\n",
        "    #     y = sum(loss_values) / num_machines\n",
        "\n",
        "\n",
        "def LBFGS(x0, pgtol=1e-5):\n",
        "    start = time.time()\n",
        "    global xs_lbfgs\n",
        "    xs_lbfgs = [x0]\n",
        "\n",
        "    print('LBFGS starts iterations')\n",
        "    x_optimal, F_at_x_optimal, info2 = fmin_l_bfgs_b(func= f_and_grad_f, x0= x0, fprime= None, args= [train_data_file, Lambda],\n",
        "                                                    approx_grad= False, bounds= [(None, None)] * number_of_features,\n",
        "                                                    pgtol= pgtol, callback= stack_steps)\n",
        "    end = time.time()\n",
        "    return end - start"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaQBILCeqPvj",
        "colab_type": "text"
      },
      "source": [
        "##One start point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsPTpJngMb8J",
        "colab": {}
      },
      "source": [
        "x_start, f_at_x_start, info = fmin_l_bfgs_b(func= f_and_grad_f, x0= np.zeros(number_of_features), fprime= None, \n",
        "                                        args= [server_data_file, Lambda], approx_grad= False, \n",
        "                                        bounds= [(None, None)] * number_of_features, maxiter= 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3pUUNiLHEzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = 1e-5\n",
        "# xs_SPAG, time_SPAG = SPAG_with_timing_and_pgtol(L_F_phi, sigma_F_phi, x_start, acc) \n",
        "xs_SPAG, time_SPAG = SPAG_con(L_F_phi, sigma_F_phi, x_start, pgtol= acc) \n",
        "ys_SPAG = find_F_at_iters(xs_SPAG, Lambda)\n",
        "\n",
        "xs_lbfgs = []\n",
        "time_LBFGS = LBFGS(x_start, pgtol= acc)\n",
        "ys_LBFGS = find_F_at_iters(xs_lbfgs, Lambda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mWPONuTHSA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Time SPAG', time_SPAG)\n",
        "print('Time LBFGS', time_LBFGS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WZawN1bnx2r",
        "colab_type": "code",
        "outputId": "231aff37-827e-4f0e-9ef9-f854c66daef4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "plt.style.use('seaborn-whitegrid')\n",
        "plt.title('Loss on iters')\n",
        "plt.xlabel(f'$iter$')\n",
        "plt.ylabel(f'$Loss$')\n",
        "\n",
        "plt.plot(ys_SPAG, 'o', color='red', label='SPAG')\n",
        "plt.plot(ys_LBFGS, 'o', color='green', label='L-BFGS')\n",
        "\n",
        "plt.legend()\n",
        "plt.savefig('Timing|_pgtol_' + str(acc)+ '.png')\n",
        "print('Timing|_pgtol_' + str(acc)+ '.png\\n')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Timing|_pgtol_1e-05.png\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}